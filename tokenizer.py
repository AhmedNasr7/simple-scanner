import re # regex lib




class Tokenizer():


    def __init__(self):
        pass


    def split_to_lines(self, input):
        pass

    def split_on_spaces(self, lines): #lines is a list
        pass

    def is_id(self, word):
        pass

    def is_digit(self, word):
        pass

    def is_keyword(self, word):
        pass

    def is_exp(self, word):
        pass

    def check_word(self, word):
        pass

    def tokenize_exp(self, exp):
        pass

    def classify(self, token):
        pass

   

